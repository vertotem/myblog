---
comments: 'true'
author: 梦貘
date: '2024-07-17 02:27 +0800'
layout: post
title: 9.11和9.9哪个大？
categories: 碎碎念
tags:
  - 碎碎念
---
## 前言

睡之前刷了一下微信公众号，看到一个[有意思的文章](https://mp.weixin.qq.com/s/BV9Xw91Tu7_hJYliH4xkNw)，说现在的大语言模型基本都不能正确回答9.11和9.9哪个大的问题，并且在文章中分析了一些可能的原因，于是我觉得有意思，就自己也试了一下。

注：由于快睡了，所以没有进行多家模型的对比测试，以下的所有测试结果都是基于OpenAI的GPT-4o模型。界面不一样是因为用的第三方客户端+API

## 正文

那篇文章里的很多网友都做了测试，也有人推测了原理，这里直接摘录

> 由于大模型以token的方式来理解文字，当9.11被拆成“9”、“小数点”和“11”三部分时，11确实比9大。
>
> 由于OpenAI使用的Tokenizer开源，可以用来观察大模型是如何理解这个问题。
>
> ![900and9-1.png]({{site.baseurl}}/images/900and9-1.png)
>
> 上图可以看出，9和小数点分别被分配为“24”和“13”，**小数点后的9同样也是“24”，而11被分配到“994”**。
>
> 所以使用这种tokenizer方法的大模型会认为9.11更大，**其实是认为11大于9**。
>
> 也有网友指出，像是书籍目录里第9.11节也比第9.9节大，所以最终可能还是训练数据里见这种见得多了，而手把手教基础算数的数据很少。
>
> 也就是问题本身对人类来说，一看就知道问的是算数问题，但对AI来说是一个模糊的问题，并不清楚这两个数字代表什么。
>
> 只要向AI解释明白这是一个**双精度浮点数**，就可以做对了。
>
> 在有额外条件的情况下，tokenizer这一步依然会给11分配更大的token。但是在后续自注意力机制的作用下，AI就会明白要把9.11连起来处理了。

原理部分我不太懂，但感觉从token的角度来解释是对的。但是文章里提出的解决办法，比如把选项放在提问前面，调换顺序就不会出错，我觉得有点玄学，所以自己动手测试了一下，结果发现了一些有趣的东西。

### 第一次提问

首先直接提问：

![900and9-2.jpg]({{site.baseurl}}/images/900and9-2.jpg)

AI觉得9.9比较小。看了文章里的猜测，有人觉得是训练数据中软件版本号和书籍章节中的9.11都比9.9大，所以这里我觉得可能是AI误解了这两个数据的类型，于是就再次提示它，这是数学概念中的小数（即原文中提的“双精度浮点数”，但是我不是数学专业的，所以就用我习惯的表达来告诉AI了）

结果AI还是回答错了。这里我猜测的原因是AI会努力保持上下文的一致性，因为上面已经回答了9.9比9.11小，所以这里污染了新的回答。为了验证这个猜想，后面我进行了第二次提问，不过这里先按下不表。

但是在AI给出的回答中，出现了上下文的逻辑不一致，它明明已经给出了小数比较大小的方法，在结论部分还是搞错了。于是我指出了它的这个矛盾，它给出了正确答案。

这里反映出来一个问题，就是AI其实是会对自己的回答做上下文一致性检测的，很多时候只要我们指出矛盾，它就会自己找到问题所在。我觉得这个可以算是一个提问的小技巧。

### 第二次提问

第一次提问中，我没有调换选项和提问的顺序，但是告诉了AI这是小数。如果在没有上文数据污染的情况下直接告诉它这是小数的比大小，会怎么样呢？

![900and9-3.jpg]({{site.baseurl}}/images/900and9-3.jpg)

可以看到这次AI直接给出了正确答案。

### 第三次提问

我在想，第一次提问中会不会是因为我知道正确答案，所以在问问题的过程中无意暗示了AI，算是变相提示了它正确答案。因为AI生成的内容是会迎合用户的想法的。

于是我决定这次直接装傻，看看效果会怎么样

![900and9-5.jpg]({{site.baseurl}}/images/900and9-5.jpg)

好玩儿的地方出现了，AI在我反复问“为什么”的情况下，自己的答案出现了反复横跳，看来它也有点被我搞迷惑了，不太清楚我想要什么样的答案。

不过最后它还是反应过来了，并且坚定的回答了正确答案。

这次提问没有什么建设性的收获，不过我多一句嘴，有了一个体会，就是这个世界总是会以我们潜意识中希望的形态出现，就像AI的回答一样。当我们迷茫的时候，世界可能也会像AI一样摸不着头脑。所以人还是应该自信大胆一点儿。先要敢想，才能得到。

跑题了。

### 第四次提问

这次提问是针对第一次提问中的“上下文一致性”进行的验证。

![900and9-6.jpg]({{site.baseurl}}/images/900and9-6.jpg)

我还是先问了AI这个问题，它给出了错误答案。然后我问它为什么，它给出了自己的思考步骤。

和第一次提问一样，这里它的思考中也出现了上下文矛盾的问题。不过这次我没有直接指出它的问题，而是让它自己进行一下上下文一致性检测。于是它就发现了自己的逻辑错误，更正了回答。

由此可见这个技巧是有用的。

## 结论

经过以上的四次提问，可以看出由于AI的技术和架构问题，它确实会对一些基础问题产生误解，但是通过一些合理的办法，也可以减少错误。

根据上文的测试以及我日常使用的总结，大概有以下几个技巧：

- 一步步思考

  这个其实就是我在上文提问中问的“为什么”，目的是让AI说出自己的思考过程。因为AI虽然逻辑性比较弱，但它其实是能发现上下文的不一致的地方的。只要让它发现了不一致的地方，它就会自己去解决。

  我们人让它说出思路，一是方便我们自己看问题出在哪儿（就算看不懂，也能发现上下文的矛盾之处），另一方面则是让AI进行自检。

  其实这个方法很好用，大语言模型刚出来那会儿有很多的prompt课都提到过这个技巧。

- 给清问题背景

  就是上文提问中的“作为数学概念中的小数”，只有AI明确了这个问题属于哪个范畴，它才能用对应的知识来解决。比如作为软件版本号的9.11和作为小数的9.11概念肯定不一样。如果不说清楚AI就会产生误解。

  其实多说一点这个就是语言的精度的问题，不光是问AI，对人说话也是这样，描述得越清楚，越可能得到清晰的解答。

- 上下文一致性

  不知道这个算不算我发现的技巧，反正之前好像没在网上看人提到过。

  就是说AI会保持上下文的一致性，所以当你发现不一致的地方的时候，直接提示它进行上下文一致性检查即可。这样AI就会自己纠错。提问时也可以直接把这句话带在末尾，让它直接自检。

## 碎碎念

其实这篇文章就是睡前的心血来潮。但我也是从OpenAI刚开始推出那个笨笨的Chat GPT就开始用，一直用到现在，也算是积累了一些经验。

其实对AI提问和对人提问是一样的，我以前在长毛象上开玩笑还说，AI用的好的人一定是个好甲方，因为和乙方沟通跟使用AI是一样的，都是要简单明确的把自己的需求描述出来。

其实上面提到的技巧用于对人体问也适用。我去年卖过prompt的课，备课的时候就一直在想，所谓的各种prompt框架只是表象，总会有不适用的场景和过时的那天。而真正的内核，其实应该是[提问的智慧](https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/main/README-zh_CN.md)。

GitHub上的那篇文章写的挺好的。

掌握了提问的智慧，才能真正掌握prompt撰写的技巧。

以上。